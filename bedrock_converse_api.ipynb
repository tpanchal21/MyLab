{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTIE35u60vlI"
      },
      "source": [
        "<h2> Converse API in Amazon Bedrock </h2>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-3lfv3rK0vlO"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall boto3\n",
        "\n",
        "import boto3\n",
        "import sys\n",
        "from botocore.exceptions import ClientError\n",
        "print('Running boto3 version:', boto3.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FYPkB80-0vlQ"
      },
      "outputs": [],
      "source": [
        "region = 'us-west-2'\n",
        "print('Using region: ', region)\n",
        "\n",
        "bedrock = boto3.client(\n",
        "    service_name = 'bedrock-runtime',\n",
        "    region_name = region,\n",
        "    )\n",
        "\n",
        "MODEL_IDS = [\n",
        "    \"amazon.titan-text-express-v1\",\n",
        "    \"amazon.titan-text-lite-v1\",\n",
        "    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
        "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
        "    \"cohere.command-r-plus-v1:0\",\n",
        "    \"cohere.command-r-v1:0\",\n",
        "    \"meta.llama3-1-70b-instruct-v1:0\",\n",
        "    \"meta.llama3-1-8b-instruct-v1:0\",\n",
        "    \"mistral.mistral-large-2407-v1:0\",\n",
        "    \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hURm6B1s0vlR"
      },
      "outputs": [],
      "source": [
        "def invoke_bedrock_model(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
        "    response = \"\"\n",
        "    try:\n",
        "        response = client.converse(\n",
        "            modelId=id,\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": [\n",
        "                        {\n",
        "                            \"text\": prompt\n",
        "                        }\n",
        "                    ]\n",
        "                }\n",
        "            ],\n",
        "            inferenceConfig={\n",
        "                \"temperature\": temperature,\n",
        "                \"maxTokens\": max_tokens,\n",
        "                \"topP\": top_p\n",
        "            }\n",
        "            #additionalModelRequestFields={\n",
        "            #}\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        result = \"Model invocation error\"\n",
        "    try:\n",
        "        result = response['output']['message']['content'][0]['text'] \\\n",
        "        + '\\n--- Latency: ' + str(response['metrics']['latencyMs']) \\\n",
        "        + 'ms - Input tokens:' + str(response['usage']['inputTokens']) \\\n",
        "        + ' - Output tokens:' + str(response['usage']['outputTokens']) + ' ---\\n'\n",
        "        return result\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        result = \"Output parsing error\"\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "dSfOG5oy0vlS"
      },
      "outputs": [],
      "source": [
        "prompt = (\"What is the capital of India?\")\n",
        "print(f'Prompt: {prompt}\\n')\n",
        "\n",
        "for i in MODEL_IDS:\n",
        "    response = invoke_bedrock_model(bedrock, i, prompt)\n",
        "    print(f'Model: {i}\\n{response}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ulpSkNgO0vlT"
      },
      "outputs": [],
      "source": [
        "def invoke_bedrock_model_stream(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
        "    response = \"\"\n",
        "    response = client.converse_stream(\n",
        "        modelId=id,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"text\": prompt\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        inferenceConfig={\n",
        "            \"temperature\": temperature,\n",
        "            \"maxTokens\": max_tokens,\n",
        "            \"topP\": top_p\n",
        "        }\n",
        "    )\n",
        "    # Extract and print the response text in real-time.\n",
        "    for event in response['stream']:\n",
        "        if 'contentBlockDelta' in event:\n",
        "            chunk = event['contentBlockDelta']\n",
        "            sys.stdout.write(chunk['delta']['text'])\n",
        "            sys.stdout.flush()\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "mJzIF9zt0vlT"
      },
      "outputs": [],
      "source": [
        "prompt = (\"What is the capital of india?\")\n",
        "print(f'Prompt: {prompt}\\n')\n",
        "\n",
        "for i in MODEL_IDS:\n",
        "    print(f'\\n\\nModel: {i}')\n",
        "    invoke_bedrock_model_stream(bedrock, i, prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "N47h0vlx0vlT"
      },
      "outputs": [],
      "source": [
        "def generate_conversation(bedrock_client,\n",
        "                          model_id,\n",
        "                          system_prompts,\n",
        "                          messages):\n",
        "    \"\"\"\n",
        "    Sends messages to a model.\n",
        "    Args:\n",
        "        bedrock_client: The Boto3 Bedrock runtime client.\n",
        "        model_id (str): The model ID to use.\n",
        "        system_prompts (JSON) : The system prompts for the model to use.\n",
        "        messages (JSON) : The messages to send to the model.\n",
        "\n",
        "    Returns:\n",
        "        response (JSON): The conversation that the model generated.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print(f'Generating message with model {model_id}')\n",
        "\n",
        "    # Inference parameters to use.\n",
        "    temperature = 0.5\n",
        "    top_k = 200\n",
        "\n",
        "    # Base inference parameters to use which are common across all FMs.\n",
        "    inference_config = {\"temperature\": temperature}\n",
        "\n",
        "    # Additional inference parameters to use for Anthropic Claude Models.\n",
        "    additional_model_fields = {\"top_k\": top_k}\n",
        "\n",
        "    # Send the message.\n",
        "    response = bedrock_client.converse(\n",
        "        modelId=model_id,\n",
        "        messages=messages,\n",
        "        system=system_prompts,\n",
        "        inferenceConfig=inference_config,\n",
        "        additionalModelRequestFields=additional_model_fields\n",
        "    )\n",
        "\n",
        "    # Log token usage.\n",
        "    token_usage = response['usage']\n",
        "    print(f\"Input tokens: {token_usage['inputTokens']}\")\n",
        "    print(f\"Output tokens: {token_usage['outputTokens']}\")\n",
        "    print(f\"Total tokens: {token_usage['totalTokens']}\")\n",
        "    print(f\"Stop reason: {response['stopReason']}\")\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "j4yXildl0vlT"
      },
      "outputs": [],
      "source": [
        "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
        "\n",
        "# Setup the system prompts and messages to send to the model.\n",
        "system_prompts = [{\"text\": \"You are an app that creates playlists for a radio station that plays rock and pop music.\"\n",
        "                    \"Only return song names and the artist.\"}]\n",
        "message_1 = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"text\": \"Create a list of 3 pop songs.\"}]\n",
        "}\n",
        "message_2 = {\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [{\"text\": \"Make sure the songs are by artists from the United Kingdom.\"}]\n",
        "}\n",
        "messages = []\n",
        "\n",
        "try:\n",
        "\n",
        "    bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
        "\n",
        "    # Start the conversation with the 1st message.\n",
        "    messages.append(message_1)\n",
        "    response = generate_conversation(\n",
        "        bedrock_client, model_id, system_prompts, messages)\n",
        "\n",
        "    # Add the response message to the conversation.\n",
        "    output_message = response['output']['message']\n",
        "    messages.append(output_message)\n",
        "\n",
        "    # Continue the conversation with the 2nd message.\n",
        "    messages.append(message_2)\n",
        "    response = generate_conversation(\n",
        "        bedrock_client, model_id, system_prompts, messages)\n",
        "\n",
        "    output_message = response['output']['message']\n",
        "    messages.append(output_message)\n",
        "\n",
        "    # Show the complete conversation.\n",
        "    for message in messages:\n",
        "        print(f\"Role: {message['role']}\")\n",
        "        for content in message['content']:\n",
        "            print(f\"Text: {content['text']}\")\n",
        "        print()\n",
        "\n",
        "except ClientError as err:\n",
        "    message = err.response['Error']['Message']\n",
        "    print(f\"A client error occured: {message}\")\n",
        "\n",
        "else:\n",
        "    print(\n",
        "        f\"Finished generating text with model {model_id}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "SlU8pJXK0vlU"
      },
      "outputs": [],
      "source": [
        "def image_conversation(bedrock_client,\n",
        "                          model_id,\n",
        "                          input_text,\n",
        "                          input_image):\n",
        "    \"\"\"\n",
        "    Sends a message to a model.\n",
        "    Args:\n",
        "        bedrock_client: The Boto3 Bedrock runtime client.\n",
        "        model_id (str): The model ID to use.\n",
        "        input text : The input message.\n",
        "        input_image : The input image.\n",
        "\n",
        "    Returns:\n",
        "        response (JSON): The conversation that the model generated.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Generating message with model {model_id}\")\n",
        "\n",
        "    # Message to send.\n",
        "\n",
        "    with open(input_image, \"rb\") as f:\n",
        "        image = f.read()\n",
        "\n",
        "    message = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"text\": input_text\n",
        "            },\n",
        "            {\n",
        "                    \"image\": {\n",
        "                        \"format\": 'jpeg',\n",
        "                        \"source\": {\n",
        "                            \"bytes\": image\n",
        "                        }\n",
        "                    }\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    messages = [message]\n",
        "\n",
        "    # Send the message.\n",
        "    response = bedrock_client.converse(\n",
        "        modelId=model_id,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "y9TeTYDT0vlU"
      },
      "outputs": [],
      "source": [
        "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
        "input_text = \"What's in this image?\"\n",
        "input_image = \"assets/sample_image.jpg\"\n",
        "\n",
        "try:\n",
        "\n",
        "    bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
        "\n",
        "    response = image_conversation(\n",
        "        bedrock_client, model_id, input_text, input_image)\n",
        "\n",
        "    output_message = response['output']['message']\n",
        "\n",
        "    print(f\"Role: {output_message['role']}\")\n",
        "\n",
        "    for content in output_message['content']:\n",
        "        print(f\"Text: {content['text']}\")\n",
        "\n",
        "    token_usage = response['usage']\n",
        "    print(f\"Input tokens:  {token_usage['inputTokens']}\")\n",
        "    print(f\"Output tokens:  {token_usage['outputTokens']}\")\n",
        "    print(f\"Total tokens:  {token_usage['totalTokens']}\")\n",
        "    print(f\"Stop reason: {response['stopReason']}\")\n",
        "\n",
        "except ClientError as err:\n",
        "    message = err.response['Error']['Message']\n",
        "    logger.error(\"A client error occurred: %s\", message)\n",
        "    print(f\"A client error occured: {message}\")\n",
        "\n",
        "else:\n",
        "    print(\n",
        "        f\"Finished generating text with model {model_id}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "wi8OlKpM0vlU"
      },
      "outputs": [],
      "source": [
        "def document_conversation(bedrock_client,\n",
        "                     model_id,\n",
        "                     input_text,\n",
        "                     input_document):\n",
        "    \"\"\"\n",
        "    Sends a message to a model.\n",
        "    Args:\n",
        "        bedrock_client: The Boto3 Bedrock runtime client.\n",
        "        model_id (str): The model ID to use.\n",
        "        input text : The input message.\n",
        "        input_document : The input document.\n",
        "\n",
        "    Returns:\n",
        "        response (JSON): The conversation that the model generated.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Generating message with model {model_id}\")\n",
        "\n",
        "    # Message to send.\n",
        "\n",
        "    with open(input_document, \"rb\") as f:\n",
        "        doc_bytes = f.read()\n",
        "\n",
        "    message = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"text\": input_text\n",
        "            },\n",
        "            {\n",
        "                \"document\": {\n",
        "                    \"name\": \"MyDocument\",\n",
        "                    \"format\": \"pdf\",\n",
        "                    \"source\": {\n",
        "                        \"bytes\": doc_bytes\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    messages = [message]\n",
        "\n",
        "    # Send the message.\n",
        "    response = bedrock_client.converse(\n",
        "        modelId=model_id,\n",
        "        messages=messages\n",
        "    )\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "iXDQAmgT0vlU"
      },
      "outputs": [],
      "source": [
        "model_id = \"meta.llama3-1-8b-instruct-v1:0\"\n",
        "input_text = \"What's in this document?\"\n",
        "input_document = 'assets/2022-Shareholder-Letter.pdf'\n",
        "\n",
        "try:\n",
        "\n",
        "    bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
        "\n",
        "    response = document_conversation(\n",
        "        bedrock_client, model_id, input_text, input_document)\n",
        "\n",
        "    output_message = response['output']['message']\n",
        "\n",
        "    print(f\"Role: {output_message['role']}\")\n",
        "\n",
        "    for content in output_message['content']:\n",
        "        print(f\"Text: {content['text']}\")\n",
        "\n",
        "    token_usage = response['usage']\n",
        "    print(f\"Input tokens:  {token_usage['inputTokens']}\")\n",
        "    print(f\"Output tokens:  {token_usage['outputTokens']}\")\n",
        "    print(f\"Total tokens:  {token_usage['totalTokens']}\")\n",
        "    print(f\"Stop reason: {response['stopReason']}\")\n",
        "\n",
        "except ClientError as err:\n",
        "    message = err.response['Error']['Message']\n",
        "    print(f\"A client error occured: {message}\")\n",
        "\n",
        "else:\n",
        "    print(\n",
        "        f\"Finished generating text with model {model_id}.\")"
      ]
    }
  ],
  "metadata": {
    "availableInstances": [
      {
        "_defaultOrder": 0,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.t3.medium",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 1,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.t3.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 2,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.t3.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 3,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.t3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 4,
        "_isFastLaunch": true,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 5,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 6,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 7,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 8,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 9,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 10,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 11,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 12,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.m5d.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 13,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.m5d.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 14,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.m5d.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 15,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.m5d.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 16,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.m5d.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 17,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.m5d.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 18,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.m5d.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 19,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.m5d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 20,
        "_isFastLaunch": false,
        "category": "General purpose",
        "gpuNum": 0,
        "hideHardwareSpecs": true,
        "memoryGiB": 0,
        "name": "ml.geospatial.interactive",
        "supportedImageNames": [
          "sagemaker-geospatial-v1-0"
        ],
        "vcpuNum": 0
      },
      {
        "_defaultOrder": 21,
        "_isFastLaunch": true,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 4,
        "name": "ml.c5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 22,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 8,
        "name": "ml.c5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 23,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.c5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 24,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.c5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 25,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 72,
        "name": "ml.c5.9xlarge",
        "vcpuNum": 36
      },
      {
        "_defaultOrder": 26,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 96,
        "name": "ml.c5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 27,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 144,
        "name": "ml.c5.18xlarge",
        "vcpuNum": 72
      },
      {
        "_defaultOrder": 28,
        "_isFastLaunch": false,
        "category": "Compute optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.c5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 29,
        "_isFastLaunch": true,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g4dn.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 30,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g4dn.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 31,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g4dn.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 32,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g4dn.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 33,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g4dn.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 34,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g4dn.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 35,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 61,
        "name": "ml.p3.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 36,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 244,
        "name": "ml.p3.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 37,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 488,
        "name": "ml.p3.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 38,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.p3dn.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 39,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.r5.large",
        "vcpuNum": 2
      },
      {
        "_defaultOrder": 40,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.r5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 41,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.r5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 42,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.r5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 43,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.r5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 44,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.r5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 45,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.r5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 46,
        "_isFastLaunch": false,
        "category": "Memory Optimized",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.r5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 47,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 16,
        "name": "ml.g5.xlarge",
        "vcpuNum": 4
      },
      {
        "_defaultOrder": 48,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.g5.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 49,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 64,
        "name": "ml.g5.4xlarge",
        "vcpuNum": 16
      },
      {
        "_defaultOrder": 50,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 128,
        "name": "ml.g5.8xlarge",
        "vcpuNum": 32
      },
      {
        "_defaultOrder": 51,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 1,
        "hideHardwareSpecs": false,
        "memoryGiB": 256,
        "name": "ml.g5.16xlarge",
        "vcpuNum": 64
      },
      {
        "_defaultOrder": 52,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 192,
        "name": "ml.g5.12xlarge",
        "vcpuNum": 48
      },
      {
        "_defaultOrder": 53,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 4,
        "hideHardwareSpecs": false,
        "memoryGiB": 384,
        "name": "ml.g5.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 54,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 768,
        "name": "ml.g5.48xlarge",
        "vcpuNum": 192
      },
      {
        "_defaultOrder": 55,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4d.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 56,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 8,
        "hideHardwareSpecs": false,
        "memoryGiB": 1152,
        "name": "ml.p4de.24xlarge",
        "vcpuNum": 96
      },
      {
        "_defaultOrder": 57,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 32,
        "name": "ml.trn1.2xlarge",
        "vcpuNum": 8
      },
      {
        "_defaultOrder": 58,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1.32xlarge",
        "vcpuNum": 128
      },
      {
        "_defaultOrder": 59,
        "_isFastLaunch": false,
        "category": "Accelerated computing",
        "gpuNum": 0,
        "hideHardwareSpecs": false,
        "memoryGiB": 512,
        "name": "ml.trn1n.32xlarge",
        "vcpuNum": 128
      }
    ],
    "instance_type": "ml.t3.medium",
    "kernelspec": {
      "display_name": "Python 3 (Data Science 3.0)",
      "language": "python",
      "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}